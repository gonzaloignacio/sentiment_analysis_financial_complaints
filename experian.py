# -*- coding: utf-8 -*-
"""Experian.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D_9k94SpA6XJkJnyKlFZdlCd2SKeM0bg
"""

# Import libraries

import pandas as pd
import string
import re
from nltk.corpus import stopwords

# Import dataframe and clean it

df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/Social Media Analytics/Experian Information Solutions INC..xlsx')
df.dropna(subset = ["Complaint text"], inplace = True)
df["Complaint text"] = df["Complaint text"].str.lower().str.strip()

# Import tools for text mining
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

#  Download dictionaries
nltk.download("punkt")
nltk.download("punkt_tab")
nltk.download("stopwords")
nltk.download("wordnet")

# Set stopwords and punctuation
stopwords = set(stopwords.words("english"))
punctuation = set(string.punctuation)


# Define function to create and clean tokens
def text_processor(text):
  text = re.sub(r'\b\w*x{2,}\w*\b', '', text)
  text = re.sub(r'\d+', '', text)
  text = re.sub(r'//', '', text)
  tokens = nltk.word_tokenize(text)
  lemmatizer = nltk.WordNetLemmatizer()
  tokens = [lemmatizer.lemmatize(token) for token in tokens]
  tokens = [token for token in tokens if token not in punctuation]
  tokens = [token for token in tokens if token not in stopwords]
  return " ".join(tokens)

# Tokenize
df["Complaint text"] = df["Complaint text"].apply(text_processor)
df

# Upload dictionary
dictionary = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Social Media Analytics/Loughran-McDonald_MasterDictionary_1993-2024.csv")

dictionary.sort_values("Litigious", ascending = False)
neg_list = dictionary[dictionary["Negative"] > 0].loc[: , "Word"].str.lower().to_list()
pos_list = dictionary[dictionary["Positive"] > 0].loc[: , "Word"].str.lower().to_list()
unc_list = dictionary[dictionary["Uncertainty"] > 0].loc[: , "Word"].str.lower().to_list()
lit_list = dictionary[dictionary["Litigious"] > 0].loc[: , "Word"].str.lower().to_list()
str_list = dictionary[dictionary["Strong_Modal"] > 0].loc[: , "Word"].str.lower().to_list()
wea_list = dictionary[dictionary["Weak_Modal"] > 0].loc[: , "Word"].str.lower().to_list()
con_list = dictionary[dictionary["Constraining"] > 0].loc[: , "Word"].str.lower().to_list()

# Define function to get sentiment scores
def sentiment_counter(text):
  negative = [1 if word in neg_list else 0 for word in text.split()]
  positive = [1 if word in pos_list else 0 for word in text.split()]
  uncertainty = [1 if word in unc_list else 0 for word in text.split()]
  litigious = [1 if word in lit_list else 0 for word in text.split()]
  strong_modal = [1 if word in str_list else 0 for word in text.split()]
  weak_modal = [1 if word in wea_list else 0 for word in text.split()]
  constraining = [1 if word in con_list else 0 for word in text.split()]
  score = {"negative": sum(negative),
           "positive": sum(positive),
           "uncertainty": sum(uncertainty),
           "litigious": sum(litigious),
           "strong_modal": sum(strong_modal),
           "weak_modal": sum(weak_modal),
           "constraining": sum(constraining)}
  return score

# Get sentiment scores
df["scores"] = df["Complaint text"].apply(sentiment_counter)

# Get the positive - negative score normalized by the length of the text
df["pos-neg score"] = df.apply(lambda x: (x["scores"]["positive"] - x["scores"]["negative"]) / len(x["Complaint text"].split()) if len(x["Complaint text"].split()) else 0, axis=1) # Check if the length of the split text is 0 before dividing. If it is 0, assign 0 to avoid ZeroDivisionError
df

# Plot histogram of the scores

import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(data = df, x = "pos-neg score", color = "darkviolet")
plt.title("Distribution of scores for Experian")
plt.xlabel("Positive-negative score")
plt.ylabel("Frequency")
plt.show()

# Classify sentiments according to the score
threshhold = df["pos-neg score"].mean() - df["pos-neg score"].std()

def sentiment_classifier(score):
  if score <= threshhold:
    return "negative"
  elif score <= 0:
    return "slightly negative"
  else:
    return "undetected"

df["sentiment class"] = df["pos-neg score"].apply(sentiment_classifier)
df.sort_values("pos-neg score")

# Plot the distribution of the sentiment categories

sns.countplot(data = df, x = "sentiment class", color = "darkviolet")
plt.title("Distribution of sentiment classes for Experian")
plt.xlabel("Sentiment class")
plt.ylabel("Frequency")
plt.show()

# Get the word cloud for undetected sentiment

from wordcloud import WordCloud
from collections import Counter

df_und = df[df["sentiment class"] == "undetected"]

all_words_undetected = " ".join(df_und["Complaint text"]).split()
word_freq_undetected = Counter(all_words_undetected)

wordcloud = WordCloud(width=800, height=400, background_color="white", max_words=20).generate_from_frequencies(word_freq_undetected)
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Most Common Words in Complaints with undetected sentiment")
plt.show()

# Get the word cloud for slightly negative sentiment

df_sli = df[df["sentiment class"] == "slightly negative"]

all_words_slightly = " ".join(df_sli["Complaint text"]).split()
word_freq_slightly = Counter(all_words_slightly)

wordcloud = WordCloud(width=800, height=400, background_color="white", max_words=20).generate_from_frequencies(word_freq_slightly)
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Most Common Words in Complaints with slightly negative sentiment")
plt.show()

# Get the word cloud for negative sentiment

df_neg = df[df["sentiment class"] == "negative"]

all_words_negative = " ".join(df_neg["Complaint text"]).split()
word_freq_negative = Counter(all_words_negative)

wordcloud = WordCloud(width=800, height=400, background_color="white", max_words=20).generate_from_frequencies(word_freq_negative)
plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Most Common Words in Complaints with negative sentiment for Expererian")
plt.show()

# Bar chart of most common words in complaints with undetected sentiment

top_undetected_words = word_freq_undetected.most_common(20)
words_und, counts_und = zip(*top_undetected_words)

sns.barplot(x = list(counts_und), y = list(words_und), color = "darkviolet")
plt.title("Most common words in complaints with undetected sentiment for Experian")
plt.xlabel("Frequency")
plt.ylabel("Word")
#plt.xticks(rotation = 45)
plt.show

# Bar chart of most common words in complaints with slightly negative sentiment

top_slightly_words = word_freq_slightly.most_common(20)
words_sli, counts_sli = zip(*top_slightly_words)

sns.barplot(x = list(counts_sli), y = list(words_sli))
plt.title("Most common words in complaints with slightly negative sentiment")
plt.xlabel("Frequency")
plt.ylabel("Word")
#plt.xticks(rotation = 45)
plt.show

# Bar chart of most common words in complaints with negative sentiment

top_negative_words = word_freq_negative.most_common(20)
words_neg, counts_neg = zip(*top_negative_words)

sns.barplot(x = list(counts_neg), y = list(words_neg), color = "darkviolet")
plt.title("Most common words in complaints with negative sentiment for Experian")
plt.xlabel("Frequency")
plt.ylabel("Word")
#plt.xticks(rotation = 45)
plt.show

"vdf.groupby("sentiment class").size()